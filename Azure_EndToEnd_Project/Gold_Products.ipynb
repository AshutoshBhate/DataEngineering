{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1611b815-201a-4696-963d-1b1e380b80d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **DLT Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "067653fd-73ae-4eb4-b8ea-b87dd45c1ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "      <div style=\"font-size:18px\">\n",
       "        The Delta Live Tables (DLT) module is not supported on this cluster.\n",
       "        You should either <a href=\"?o=103552896721833#joblist/pipelines/create?initialSource=%2FUsers%2Fashutoshbhate7%40gmail.com%2FDatabricks_ETE_Project%2FGold_Products&redirectNotebookId=957190749451873\">create a new pipeline</a> or use an existing pipeline to run DLT code.\n",
       "      </div>\n",
       "    </html>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mDLTImportException\u001B[0m                        Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5919758866350932>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/PostImportHook.py:231\u001B[0m, in \u001B[0;36m_ImportHookChainedLoader.exec_module\u001B[0;34m(self, module)\u001B[0m\n",
       "\u001B[1;32m    229\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexec_module\u001B[39m(\u001B[38;5;28mself\u001B[39m, module):\n",
       "\u001B[1;32m    230\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 231\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader\u001B[38;5;241m.\u001B[39mexec_module(module)\n",
       "\u001B[1;32m    232\u001B[0m         notify_module_loaded(module)\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mImportError\u001B[39;00m, \u001B[38;5;167;01mAttributeError\u001B[39;00m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/dlt/__init__.py:22\u001B[0m\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# DLT python module is not supported when Spark Connect is enabled. So, we throw an error\u001B[39;00m\n",
       "\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# here.\u001B[39;00m\n",
       "\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_remote_spark:\n",
       "\u001B[0;32m---> 22\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DLTImportException(\n",
       "\u001B[1;32m     23\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDelta Live Tables module is not supported on Spark Connect clusters. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     24\u001B[0m     )\n",
       "\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
       "\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mhelpers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deprecation_warn\n",
       "\n",
       "\u001B[0;31mDLTImportException\u001B[0m: Delta Live Tables module is not supported on Spark Connect clusters. "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "DLTImportException",
        "evalue": "Delta Live Tables module is not supported on Spark Connect clusters. "
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>DLTImportException</span>: Delta Live Tables module is not supported on Spark Connect clusters. "
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mDLTImportException\u001B[0m                        Traceback (most recent call last)",
        "File \u001B[0;32m<command-5919758866350932>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/PostImportHook.py:231\u001B[0m, in \u001B[0;36m_ImportHookChainedLoader.exec_module\u001B[0;34m(self, module)\u001B[0m\n\u001B[1;32m    229\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexec_module\u001B[39m(\u001B[38;5;28mself\u001B[39m, module):\n\u001B[1;32m    230\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 231\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader\u001B[38;5;241m.\u001B[39mexec_module(module)\n\u001B[1;32m    232\u001B[0m         notify_module_loaded(module)\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mImportError\u001B[39;00m, \u001B[38;5;167;01mAttributeError\u001B[39;00m):\n",
        "File \u001B[0;32m/databricks/spark/python/dlt/__init__.py:22\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# DLT python module is not supported when Spark Connect is enabled. So, we throw an error\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# here.\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_remote_spark:\n\u001B[0;32m---> 22\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DLTImportException(\n\u001B[1;32m     23\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDelta Live Tables module is not supported on Spark Connect clusters. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     24\u001B[0m     )\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mhelpers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deprecation_warn\n",
        "\u001B[0;31mDLTImportException\u001B[0m: Delta Live Tables module is not supported on Spark Connect clusters. "
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86c2d581-8a8e-4a6b-8885-546fc21beeb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Streaming Table - We create streaming table on a source (products_silver)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d86c7f1b-4436-460c-9aac-3bc37d8dd0b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Expectations\n",
    "my_rules = {\n",
    "    \"rule1\" : \"product_id IS NOT NULL\",\n",
    "    \"rule2\" : \"product_name IS NOT NULL\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60f60617-4c31-4996-a782-f0d4824cb862",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5919758866350928>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;129m@dlt\u001B[39m\u001B[38;5;241m.\u001B[39mtable(name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDimProducts\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      2\u001B[0m \n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mDimProducts_stage\u001B[39m():\n",
       "\u001B[1;32m      4\u001B[0m   df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mreadStream\u001B[38;5;241m.\u001B[39mtable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdatabricks_cata.products_silver\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      5\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'dlt' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NameError",
        "evalue": "name 'dlt' is not defined"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'dlt' is not defined"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5919758866350928>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;129m@dlt\u001B[39m\u001B[38;5;241m.\u001B[39mtable(name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDimProducts\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mDimProducts_stage\u001B[39m():\n\u001B[1;32m      4\u001B[0m   df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mreadStream\u001B[38;5;241m.\u001B[39mtable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdatabricks_cata.products_silver\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
        "\u001B[0;31mNameError\u001B[0m: name 'dlt' is not defined"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dlt.table()\n",
    "\n",
    "@dlt.expect_all_or_drop(my_rules)\n",
    "def DimProducts_stage():\n",
    "  df = spark.readStream.table(\"databricks_cata.silver.products_silver\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "180fdaf9-3247-498c-b9bc-52e28aa7fb95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Streaming View**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf4cf8a4-8eb0-48a2-93a1-30c5e4e4eb09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.view\n",
    "\n",
    "def DimProducts_view():\n",
    "    df = spark.readStream.table(\"Live.DimProducts_stage\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87d3cd24-c480-46a8-8a8a-e51e09b6936b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**DimProducts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20cdf6f3-e8ff-4449-947d-03980bd7c7ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dlt.create_streaming_table(\"DimProducts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7f4bab6-f151-420b-990f-f4ff738dfeaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dlt.apply_changes(\n",
    "    target = \"DimProducts\",\n",
    "    source = \"Live.DimProducts_view\",\n",
    "    keys = [\"product_id\"],\n",
    "    sequence_by = \"product_id\",\n",
    "    stored_as_scd_type = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ca0121b-57c0-4e40-be04-b64bdad1de08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gold_Products",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}